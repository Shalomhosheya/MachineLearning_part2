{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e12f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-DF matrix created successfully\n",
      "Shape of the matrix: (9, 1000) \n",
      "\n",
      "Cluster labels for training data: [0 0 1 0 2]\n",
      "Predicted cluster for new data: [0]\n"
     ]
    }
   ],
   "source": [
    "import wikipediaapi\n",
    "import nltk\n",
    "import ssl\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "try:\n",
    "              _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "             pass\n",
    "else:\n",
    "             ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# # Now, try to download the data again\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "# print(\"NLTK data downloaded successfully using the manual method.\")\n",
    "\n",
    "wiki_api = wikipediaapi.Wikipedia('text_pr/1.0','en')\n",
    "page = wiki_api.page('Elon Musk')\n",
    "\n",
    "# if page.exists():\n",
    "#     print(\"title\",page.title,\"\\n\")\n",
    "#     print(\"text\",page.text,\"\\n\")\n",
    "# else:\n",
    "#     print(\"Page does not exist\")\n",
    "\n",
    "\n",
    "article_title = [\n",
    "    \"Galaxy\",\"Black hole\",\"Supernova\",\n",
    "    \"DNA\",\"Photosynthesis\",\"Evolution\",\n",
    "    \"Computer Programming\",\"Artificial Intelligence\",\"Machine Learning\",\n",
    "]\n",
    "\n",
    "wiki_api = wikipediaapi.Wikipedia('MyClusteringProjecct/1.0','en')\n",
    "documents = []\n",
    "\n",
    "for title in article_title:\n",
    "    page = wiki_api.page(title)\n",
    "    if page.exists():\n",
    "        documents.append(page.text)\n",
    "        \n",
    "# print(documents,\"\\n\")\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocess function\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    processed_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    # Join back into a string\n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "# Example: process a list of documents\n",
    "processed_documents = [preprocess_text(doc) for doc in documents]\n",
    "\n",
    "# for i in processed_documents:\n",
    "#     print(i) \n",
    "#     print(\" \")\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_documents)\n",
    "\n",
    "# print(tfidf_matrix)\n",
    "\n",
    "\n",
    "print(\"TF-DF matrix created successfully\")\n",
    "print(f\"Shape of the matrix: {tfidf_matrix.shape}\",'\\n')\n",
    "\n",
    "\n",
    "# Example training documents\n",
    "documents = [\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"KMeans is an unsupervised machine learning algorithm.\",\n",
    "    \"Data science involves statistics and programming.\",\n",
    "    \"Artificial intelligence includes machine learning.\",\n",
    "    \"Clustering groups similar data points together.\"\n",
    "]\n",
    "\n",
    "# Step 1: Vectorize training data\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Step 2: Train KMeans\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "kmeans.fit(tfidf_matrix)\n",
    "\n",
    "# Step 3: Check labels for training data\n",
    "labels = kmeans.labels_\n",
    "print(\"Cluster labels for training data:\", labels)\n",
    "\n",
    "# Step 4: Predict cluster for new data\n",
    "new_data = [\n",
    "    \"An algorithm is a set of well-defined instructions designed to perform a specific task.\"\n",
    "]\n",
    "\n",
    "# Transform new data using the SAME vectorizer\n",
    "new_data_tfidf = vectorizer.transform(new_data)\n",
    "\n",
    "# Predict cluster\n",
    "pred = kmeans.predict(new_data_tfidf)\n",
    "print(\"Predicted cluster for new data:\", pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0917f85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
