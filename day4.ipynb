{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "79e12f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 5188 stored elements and shape (9, 1000)>\n",
      "  Coords\tValues\n",
      "  (0, 380)\t0.8693801427182205\n",
      "  (0, 905)\t0.011595619786716945\n",
      "  (0, 872)\t0.2833123787771837\n",
      "  (0, 877)\t0.02499815106857503\n",
      "  (0, 778)\t0.0049082727371812275\n",
      "  (0, 475)\t0.02699550005449675\n",
      "  (0, 383)\t0.06292594729631627\n",
      "  (0, 271)\t0.029449636423087365\n",
      "  (0, 216)\t0.03926618189744982\n",
      "  (0, 553)\t0.03545776891403571\n",
      "  (0, 108)\t0.008759479748609945\n",
      "  (0, 931)\t0.010637330674210712\n",
      "  (0, 401)\t0.016665434045716687\n",
      "  (0, 992)\t0.005318665337105356\n",
      "  (0, 572)\t0.13610104470668627\n",
      "  (0, 762)\t0.002898904946679236\n",
      "  (0, 982)\t0.07102317119364128\n",
      "  (0, 194)\t0.005318665337105356\n",
      "  (0, 854)\t0.01313921962291492\n",
      "  (0, 309)\t0.009832179265049416\n",
      "  (0, 573)\t0.0159439772067358\n",
      "  (0, 741)\t0.004806502600056576\n",
      "  (0, 846)\t0.01950177290271964\n",
      "  (0, 272)\t0.055551446819055626\n",
      "  (0, 515)\t0.00724726236669809\n",
      "  :\t:\n",
      "  (8, 25)\t0.09591686410976936\n",
      "  (8, 752)\t0.008719714919069942\n",
      "  (8, 756)\t0.017439429838139883\n",
      "  (8, 74)\t0.004359857459534971\n",
      "  (8, 605)\t0.10899643648837427\n",
      "  (8, 875)\t0.04359857459534971\n",
      "  (8, 310)\t0.004359857459534971\n",
      "  (8, 801)\t0.008719714919069942\n",
      "  (8, 575)\t0.056678146973954624\n",
      "  (8, 455)\t0.021799287297674854\n",
      "  (8, 954)\t0.06975771935255953\n",
      "  (8, 898)\t0.07411757681209451\n",
      "  (8, 766)\t0.07411757681209451\n",
      "  (8, 248)\t0.04359857459534971\n",
      "  (8, 614)\t0.008719714919069942\n",
      "  (8, 943)\t0.004359857459534971\n",
      "  (8, 83)\t0.03923871713581474\n",
      "  (8, 152)\t0.030519002216744796\n",
      "  (8, 493)\t0.03923871713581474\n",
      "  (8, 606)\t0.056678146973954624\n",
      "  (8, 935)\t0.06539786189302456\n",
      "  (8, 804)\t0.013079572378604912\n",
      "  (8, 707)\t0.013079572378604912\n",
      "  (8, 345)\t0.004359857459534971\n",
      "  (8, 189)\t0.04359857459534971\n",
      "TF-DF matrix created successfully\n",
      "Shape of the matrix: (9, 1000) \n",
      "\n",
      "[2 2 2 0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import wikipediaapi\n",
    "import nltk\n",
    "import ssl\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "try:\n",
    "              _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "             pass\n",
    "else:\n",
    "             ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# # Now, try to download the data again\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "# print(\"NLTK data downloaded successfully using the manual method.\")\n",
    "\n",
    "wiki_api = wikipediaapi.Wikipedia('text_pr/1.0','en')\n",
    "page = wiki_api.page('Elon Musk')\n",
    "\n",
    "# if page.exists():\n",
    "#     print(\"title\",page.title,\"\\n\")\n",
    "#     print(\"text\",page.text,\"\\n\")\n",
    "# else:\n",
    "#     print(\"Page does not exist\")\n",
    "\n",
    "\n",
    "article_title = [\n",
    "    \"Galaxy\",\"Black hole\",\"Supernova\",\n",
    "    \"DNA\",\"Photosynthesis\",\"Evolution\",\n",
    "    \"Computer Programming\",\"Artificial Intelligence\",\"Machine Learning\",\n",
    "]\n",
    "\n",
    "wiki_api = wikipediaapi.Wikipedia('MyClusteringProjecct/1.0','en')\n",
    "documents = []\n",
    "\n",
    "for title in article_title:\n",
    "    page = wiki_api.page(title)\n",
    "    if page.exists():\n",
    "        documents.append(page.text)\n",
    "        \n",
    "# print(documents,\"\\n\")\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocess function\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    processed_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    # Join back into a string\n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "# Example: process a list of documents\n",
    "processed_documents = [preprocess_text(doc) for doc in documents]\n",
    "\n",
    "# for i in processed_documents:\n",
    "#     print(i) \n",
    "#     print(\" \")\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_documents)\n",
    "\n",
    "print(tfidf_matrix)\n",
    "\n",
    "\n",
    "print(\"TF-DF matrix created successfully\")\n",
    "print(f\"Shape of the matrix: {tfidf_matrix.shape}\",'\\n')\n",
    "\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k,random_state=42,n_init=10)\n",
    "\n",
    "kmeans.fit(tfidf_matrix)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "print(labels)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
